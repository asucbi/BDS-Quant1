---
title: "Module 13: APA Summary for Multilevel Model"
format: html
editor: visual
---

```{r, message=FALSE, echo=FALSE, warning=FALSE}

library(tidyverse)
library(ggrain) #for making raincloud plots
library(easystats) #to get ICC
library(lme4) #for running multilevel (aka mixed-effects) models
library(lmerTest) #for generating test statistics for multilevel models
library(partR2) #for generating semi-partial R^2
library(performance) #for examining model assumptions

nels <- read_csv("data/NELS88.csv")

nels <- nels %>% 
  mutate(Schoolid = as.factor(Schoolid),
         schooltype = as.factor(schooltype))

summary_stats <- nels %>%
  group_by(Schoolid) %>%
  summarise(n_students = n()) %>%
  ungroup() %>%
  summarise(
    n_clusters = n(),
    total_students = sum(n_students),
    average_students_per_classroom = mean(n_students),
    SD_students_per_classroom = sd(n_students),
    min_students_in_classroom = min(n_students),
    max_students_in_classroom = max(n_students)
  )

# Extracting the values to variables for easier printing later
n_clusters <- summary_stats$n_clusters
total_students <- summary_stats$total_students
average_students <- summary_stats$average_students_per_classroom
sd_students <- summary_stats$SD_students_per_classroom
min_students <- summary_stats$min_students_in_classroom
max_students <- summary_stats$max_students_in_classroom

#lots of ways to do this, first get means for each school
school_means <- nels %>% 
  group_by(Schoolid) %>% 
  summarise(mean_time = mean(timeonmath, na.rm = TRUE))

#join with nels data and then center
nels <- nels %>% 
  left_join(school_means) %>% 
  mutate(timeonmath_c = timeonmath - mean_time)

#fit the null model
null <- lmer(mathscore ~ (1|Schoolid), data = nels)

#now, let's use the `icc` function in `{easystats}` to generate the ICC.
ICC <- icc(null)

model_1 <- lmer(mathscore ~ timeonmath_c + schooltype + (1|Schoolid), data = nels)

model_2 <- lmer(mathscore ~ timeonmath_c * schooltype + (1|Schoolid), data = nels)

lrt1 <- test_likelihoodratio(model_1, model_2, REML=FALSE)

model_3 <- lmer(mathscore ~ timeonmath_c * schooltype + (timeonmath_c|Schoolid), data = nels)

lrt2 <- test_likelihoodratio(model_2, model_3)

#pseudo R^2 (conditional = full model, marginal = fixed effects only)
pseudo_r2 <- r2(model_3)

#cohen's d = effect / sqrt(var(int) + var(slope) + var(resid))
cohensd <- -15.605 / sqrt(9.683+25.577+25.577)

```

The present study examined the effect of school type on the relation between time spent on math homework and math scores, using a nested data structure with two levels: students (Level 1) nested within classrooms (Level 2). The analysis included `r summary_stats$total_students` distributed across `r summary_stats$n_clusters` classrooms. Classrooms varied in size, with an average of approximately `r summary_stats$average_students_per_classroom` students per classroom ($SD$ = `r round(summary_stats$SD_students_per_classroom, 2)`, $range$ = \[`r summary_stats$min_students_in_classroom`, `r summary_stats$max_students_in_classroom`\]).

The effect of school type on the relation between time spent on math homework and math scores was modeled using a multilevel linear model where `mathscore` is the students' score on a mathematics assessment, `schooltype` indicates whether the school was public or private, `timonmath` is the number of hours the student spends on math per week, and (`timeonmath|schoolid`) represents random intercepts and slopes for `timeonmath` across schools. We group mean centered `timeonmath` (around the mean for each school) for our analyses.

A maximal model, including main effects and interactions for school type and time spent on math as well as a random intercept for school and a random slopes for school type and time spent on math failed to converge. Before settling on a final model, we conducted a likelihood ratio test (using the `{easystats}` package, version 0.7.3) to compare three possible models; (1) a model that included only main effects for school type and time spent on math as well as a random intercept for school ($df$ = `r lrt1$df[1]` (2) a model that included main effects and interactions for school type and time spent on math as well as a random intercept for school ($df$ = `r lrt1$df[2]`), and (3) a model that additionally included a random slope for time spent on math ($df$ = `r lrt2$df[2]`). We found that the model that included the interaction term significantly (model 2) improved model fit over a model without the interaction (model 1), $\chi^2$ = `r round(lrt1$Chi2[2], 2)`, $p$ `r papaja::printp(lrt1$p[2], add_equals = TRUE)`. However, the most complex model (model 3) additionally significantly improved model fit over model 2, $\chi^2$ = `r round(lrt2$Chi2[2], 2)`, $p$ `r papaja::printp(lrt2$p[2], add_equals = TRUE)`.

We checked our model assumptions (linearity, normality of residuals, homoscedasticity, collinearity) visually, using diagnostic plots from the performance package (version 0.13.0). Based on this visual assessment, we found that all our model assumptions were satisfied. The Intraclass Correlation Coefficient (ICC) for the outcome variable was calculated as `r round(ICC$ICC_conditional, 2)`, suggesting that `r round(ICC$ICC_conditional, 2)*100`% of the variance in math scores could be attributed to differences between classrooms. The models were estimated using Restricted Maximum Likelihood (REML) to account for the nested data structure. The lme4 package (version 1.1.35) was used for fitting the model.

```{r message= FALSE, echo=FALSE}

#get means
nels_desc <- nels %>% 
  group_by(schooltype) %>% 
  summarise(mean = mean(mathscore, na.rm = TRUE),
            sd = sd(mathscore, na.rm = TRUE))

```
Overall, our final model explained `r round(pseudo_r2$R2_conditional, 2)*100`% of the variance in math scores (marginal $R^2$ = `r round(pseudo_r2$R2_marginal, 2)*100`%). 

Our fixed effects analysis revealed a significant intercept, $\beta$ = `r round(summary(model_3)$coefficients["(Intercept)", "Estimate"], 2)`, $SE$ = `r round(summary(model_3)$coefficients["(Intercept)", "Std. Error"], 2)`), $p$ `r papaja::printp(summary(model_3)$coefficients["(Intercept)", "Pr(>|t|)"], add_equals = TRUE)`, indicating a high baseline level of math scores. The main effect of time on math was not significant, $\beta$ = `r round(summary(model_3)$coefficients["timeonmath_c", "Estimate"], 2)`, $SE$ = `r round(summary(model_3)$coefficients["timeonmath_c", "Std. Error"], 2)`), $p$ `r papaja::printp(summary(model_3)$coefficients["timeonmath_c", "Pr(>|t|)"], add_equals = TRUE)` nor was the interaction between time spent on my and school type $\beta$ = `r round(summary(model_3)$coefficients["timeonmath_c:schooltypepublic", "Estimate"], 2)`, $SE$ = `r round(summary(model_3)$coefficients["timeonmath_c:schooltypepublic", "Std. Error"], 2)`), $p$ `r papaja::printp(summary(model_3)$coefficients["timeonmath_c:schooltypepublic", "Pr(>|t|)"], add_equals = TRUE)`. The main effect of school type was significant, $\beta$ = `r round(summary(model_3)$coefficients["timeonmath_c", "Estimate"], 2)`, $SE$ = `r round(summary(model_3)$coefficients["timeonmath_c", "Std. Error"], 2)`), $p$ `r papaja::printp(summary(model_3)$coefficients["timeonmath_c", "Pr(>|t|)"], add_equals = TRUE)` such that private schools ($mean$ = `r round(nels_desc$mean[1], 2)`, $sd$ = `r round(nels_desc$sd[1], 2)`) had higher math scores than public schools ($mean$ = `r round(nels_desc$mean[2], 2)`, $sd$ = `r round(nels_desc$sd[2], 2)`). However, only one of the schools in our sample was private.

Regarding random effects, we observed a variance of `r round(summary(model_3)$varcor$Schoolid[1],2)` ($SD$ = `r round(sqrt(summary(model_3)$varcor$Schoolid[1]),2)`) for the intercepts across different classrooms. This suggests that there is  variability in the baseline math scores that can be attributed to differences between classrooms before considering time spent on math or school type. The variance associated with the effect of time spent on math `r round(as.numeric(VarCorr(model_3)$Schoolid["timeonmath_c", "timeonmath_c"]),2)` ($SD$ = `r round(sqrt(as.numeric(VarCorr(model_3)$Schoolid["timeonmath_c", "timeonmath_c"])),2)`). This suggests that there is variability in how the effect of time on math on math scores differs across classrooms. Finally, the variance of residuals was `r round(summary(model_3)$sigma^2, 2)` ($SD$ = `r round(sqrt(summary(model_3)$sigma^2), 2)`), indicating that within-classroom variance in math scores cannot be fully explained by the model's fixed effects or the classroom-level random effects. Together, this suggests that multilevel modeling is, indeed, appropriate and necessary to account for the clustering of student scores within classrooms and the variability in the effect of time spent on math across classrooms.The correlation between the random intercepts and slopes for time spent on math within classrooms was estimated to be $r$ = `r round(attr(VarCorr(model_3)$Schoolid, "correlation")[1, 2], 2)`, suggesting a positive relationship between the classroom baseline scores and the effect of time on math.

Taken together, these results suggest that school type, but not time spent on math homework or the interaction between time on math and school type, is a significant predictor of math scores. These findings highlight the importance of considering school-level effects on student performance. 

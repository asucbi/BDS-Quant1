---
title: "Module 4: Lab Instructions (Instructor File)"
format: html
editor: visual
---

# Purpose

In today's lab, we will review Multiple Regression, work with Partial and Semi-Partial Correlations, and practice significance testing for Multiple Regression.

For today's lab, you will need to load the following libraries.

```{r libraries, message=FALSE}

library(tidyverse)
library(papaja) #for formatting p values
library(broom) #for formatting tables
library(knitr) #for formatting tables
library(ppcor) #for calculating partial and semi-partial correlations
library(easystats) #for assessing multicollinaerity

```

## Read in the Data

First, let's read in the data and then use `View()` to check it out.

Today's dataset is called `data_htwtage.csv`. Let's read it in and name it `mr_data` (i.e., multiple regression data).

```{r read-and-view, message= FALSE, echo=FALSE}

mr_data <- read_csv("data/data_htwtage.csv")

View(mr_data)

```

The data has 4 columns:

-   name: a (not-anonymous!) participant identifier
-   age: participant age
-   height: participants' height
-   weight: participants' weight

(pretty self-explanatory!)

## Visualizing Relations

We should always start by looking at our data.

## Scatter plot

Scatter plots allow us to visualize the relationship between two variables. Let's create two scatter plots of the relation between our IVs and DV using `ggplot()`.

```{r scatter-plot, message = FALSE, echo=FALSE, eval=FALSE}

ggplot(data = mr_data, aes(x=age, y=weight)) +
  geom_point() +
  labs(x = "Age", y = "Weight") #label your axes!

ggplot(data = mr_data, aes(x=height, y=weight)) +
  geom_point() +
  labs(x = "Height", y = "Weight") #label your axes!

```

> **Question:** What do you notice about the strength and direction of the data?

INST ANS: Both age and height seem to be positively correlated with weight.

# Multiple Regression

Let's use both `age` and `height` to predict `weight`.

Regress `weight` on `age` and `height`. Save the model to an object called `model_1`.

```{r run-regression, message = FALSE, echo=FALSE, eval=FALSE}

model_1 <- lm(weight ~ age + height, data = mr_data)
summary(model_1)

```

The results provide us with the following model:

$$W_i = -141.22 + 1.28A_i + 3.60H_i + \epsilon_i$$

> **Question** How do we interpret each of these regression coefficients?

INST ANS: When age and height are 0 the predicted weight is -141.22. For each one-unit increase in age we expect weight to increase by 1.28, holding height constant. For each one-unit increase in height we expect weight to increase by 3.60, holding age constant.

Does this intercept make sense?

# Partial Regression Coefficients

The regression coefficients corresponding to the predictors in a multiple regression model are called *partial regression coefficients*. Let's break down what this term means.

A new concept that gets introduced when we have multiple predictors in the regression model is *redundancy*, which occurs when the predictors in a multiple regression model are correlated (which they often are) and there is overlap in the variability each can account for in the outcome variable. In other words, some of the variability X1 can account for in Y is *redundant* with the variability X2 can account for in Y.

<center>![](images/partial_reg_coeff_image.png)</center>

The *partial regression coefficients* that we obtain in the "Estimate" column from the summary output of our multiple regression model account for this redundancy. A partial regression coefficient is the relationship between a predictor variable (X1) and outcome variable (Y) when the relationship between X1 and the other predictor variable(s) in the model has been removed.

Let's walk through the logic of a partial regression coefficient by re-creating the partial regression coefficient corresponding to `age` ($b_1$ = 1.28) from our multiple regression model above.

INST NOTE: DRAW PARTIAL AND SEMI-PARTIAL CORRELATIONS ON THE BOARD

# Re-creating the partial regression coefficient for `age`

1.  Regress `age` on `height`. Store the model in an object called `model_2`

```{r, message= FALSE, echo=FALSE}

model_2 <- lm(age ~ height, data = mr_data)

```

2.  Store the part of `age` that is unrelated to `height` in a new column in `data` called `age_resid`.

```{r, message= FALSE, echo=FALSE}

mr_data$age_resid <- resid(model_2)

```

3.  Regress `weight` on the part of `age` that is unrelated to `height` (i.e., `data$age_resid`). Store the model in an object called `model_3`.

```{r, message= FALSE, echo=FALSE}

model_3 <- lm(weight ~ age_resid, data = mr_data)

```

4.  Examine the final results using the `summary()` function.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

summary(model_3)

```

> **Question** How does the regression coefficient corresponding to the part of `age` that is unrelated to `height` in the univariate model (i.e., `model_3`) predicting `weight` compare to the regression coefficient corresponding to `age` in the multiple regression model (i.e., `model_1`)?

INST ANS: It's the same thing!

> **Question** Which type of correlation is the partial regression coefficient conceptually most similar to?

INST ANS: Semi-partial correlations (because we're looking at prop of variance explained by age out of entire variance in weight and not variance that is just left over after accounting for the other variables)

# Semi-Partial and Partial Correlations

You can also use functions in the `{ppcor}` package to calculate semi-partial and partial correlations more quickly.

## Semi-partial Correlation

Here, we will calculate the **semi-partial** correlation for age using the `spcor.test` function.

```{r, message= FALSE, echo=FALSE, eval=FALSE}
semi_partial_cor <- spcor.test(
  x = mr_data$weight, # Y (var of interest 1)
  y = mr_data$age,    # X1 (var of interest 2 - the one you want residualized)
  z = mr_data$height  # X2 (control variable)
)

semi_partial_cor
semi_partial_cor$estimate^2 # R-Squared 

```

## Partial Correlation

Here, we will calculate the **partial** correlation for age using the `spcor.test` function.

```{r, message= FALSE, echo=FALSE, eval=FALSE}
partial_cor <- pcor.test(
  x = mr_data$weight, # Y (var of interest 1)
  y = mr_data$age,    # X1 (var of interest 2)
  z = mr_data$height  # X2 (control variable)
)

partial_cor
partial_cor$estimate^2 # R-Squared

```

# Significance Testing: The Model Comparison Approach

## Testing the Significance of the Overall Model

A helpful way of framing the null hypothesis we want to test is using the model comparison approach. In this approach, you construct two models. One model represents what you would expect if the null hypothesis was true, and one model represents the alternative hypothesis. The model representing the alternative hypothesis includes the predictor(s) you want to test the significance of. The model representing the null hypothesis does not include these predictor(s). These predictors should be the *only* thing that differs between the two models.

Testing the significance of the overall model (AKA, the omnibus test), means examining whether, as a set, both `age` and `height` account for a significant amount of variation in `weight` scores.

We can test this by comparing two different models:

$$Model 1: W_i = \beta_0 + \epsilon_i$$

$$Model 2: W_i = \beta_0 + \beta_1A_i + \beta_2H_i + \epsilon_i$$

Where the null hypothesis states:

$$H0: \beta_1 = \beta_2 = 0$$

And the alternative hypothesis states:

$$H1: \beta_1 \neq 0 \; \textrm{AND/OR} \; \beta_2 \neq 0 $$

The key to the *model comparison approach* is comparing the amount of unaccounted for error remaining when Model 1 is used compared to when Model 2 is used. Then, we can test whether the difference in the amount of unaccounted for error remaining between the two models is significant.

Let's run Model 1 (the intercept-only model) and see how much error is left unaccounted for. Save Model 1 to `model_1`.

```{r, message= FALSE, echo=FALSE}

model_1 <- lm(weight ~ 1, data = mr_data)

```

Let me introduce a new way of looking at the output of a regression analysis, the `anova()` function. This function gives us the SSE (the amount of unaccounted for error remaining in Model 1).

```{r, message= FALSE, echo=FALSE, eval=FALSE}

model_1_anova <- anova(model_1)
model_1_anova

```

The Sum of Squares on the *residuals* row corresponds to the SSE. That is, if you were to calculate the distance between each person's actual weight and the weight predicted by the model (in this case, the mean), square those distances, and sum them up, you would get 9335.737.

Let's run Model 2 (now including age and height) and see how much error is left unaccounted for. Save Model 2 to `model_2`.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

model_2 <- lm(weight ~ age + height, data = mr_data)
model_2_anova <- anova(model_2)
model_2_anova

```

Look at the residuals row again to get the SSE. The amount of unaccounted for error remaining when we use model 2 is 2120.1. An improvement from Model 1!

How much was SSE reduced by using Model 2 instead of Model 1? Save the output to a variable called `ssr` (i.e., sum of squares reduced).

```{r, message= FALSE, echo=FALSE, eval=FALSE}

model_1_sse <- model_1_anova$`Sum Sq`[1]
model_2_sse <- model_2_anova$`Sum Sq`[3]

ssr <- model_1_sse - model_2_sse
ssr

```

## Model Comparison in a Single Step Using the `anova()` function

We can give both models as arguments to the `anova()` function to make the model comparison in a single step and test whether the reduction in SSE is significant. Save the results of the model comparison to an object called `model_comparison`.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

model_comparison <- anova(model_1, model_2)
model_comparison

```

Let's also calculate how much we reduced the amount of unaccounted for error using Model 2 compared to Model 1 as a proportion of the total amount of unaccounted for error we started with in Model 1. Save the output to a variable called `per` (i.e., proportion_error_reduced).

```{r, message= FALSE, echo=FALSE, eval=FALSE}

per <- (model_1_sse - model_2_sse) / model_1_sse
per

```

> **Question** What does this proportional reduction in error mean?

INST ANS: Age and height explained 77.29% of the variability in weight scores.

> **Question** What is the more familiar term for this value?

INST ANS: R-squared; coefficient of determination.

Look again at our summary output if we run our multiple regression model (i.e., `model_2`) and see how our results map on.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

summary(model_2)

```

The statistics given at the bottom of the summary output correspond to a test of the significance of the *overall model* i.e., the *omnibus test*.

# Significance Testing: The Model Comparison Approach

## Testing the Significance of Specific Predictor(s)

We can also construct a model comparison to represent the null hypothesis we want to test to see if a specific predictor in the model is significant. Let's do an example testing the significance of Height in our multiple regression model.

$$Model 1: W_i = \beta_0 + \beta_1A_i + \epsilon_i$$

$$Model 2: W_i = \beta_0 + \beta_1A_i + \beta_2H_i + \epsilon_i$$

Where the null hypothesis states:

$$H0: \beta_2 = 0$$

And the alternative hypothesis states:

$$H1: \beta_2 \neq 0$$

Construct Model 1 and Model 2. Store in variables labelled `model_1` and `model_2`, respectively.

```{r, message= FALSE, echo=FALSE}

model_1 <- lm(weight ~ age, data = mr_data)
model_2 <- lm(weight ~ age + height, data = mr_data)

```

Compare the models using the `anova()` function. Save the results of the model comparison to an object called `model_comparison`.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

model_comparison <- anova(model_1, model_2)
model_comparison

```

Compute $R^2$.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

r_squared <- model_comparison$`Sum of Sq`[2] / model_comparison$RSS[1]
r_squared

```

Look at summary output from Model 2 again.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

summary(model_2)

```

There's equivalence between what we've done using the model comparison approach and the *t*-test approach that's used to test the significance of the specific predictors in the model in the summary output. Notice what we get if we take the square root of the F-statistic from the model comparison output.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

sqrt(model_comparison$F[2])

```

And also notice the correspondence in the p-values.

```{r, message= FALSE, echo=FALSE, eval=FALSE}

model_comparison$`Pr(>F)`[2]

```

# Testing for Multicollinaerity

> **Question** Do you think there is likely to be multicollinaerity in this dataset? Why?

INST ANS: YES!

## Let's identify potential problems with multicollinaerity.

First, let's create a correlation matrix to examine potential correlations between our variables. Make sure to select only the variables that you want to examine.

```{r correlation-matrix, message = FALSE, echo=FALSE, eval=FALSE}

cor_df <- mr_data[,2:4] #select only the numeric variables
cor(cor_df) #create a correlation matrix

```

Next, let's use the `easystats` package to check the Tolerance and VIF (Variance Inflation Factor) in our data. Use `model_2` that you created above.

```{r check-multicollinaerity, message = FALSE, echo=FALSE, eval=FALSE}

check_collinearity(model_2)


```

> **Question** Does it look like we have multicollinaerity in our data based on these diagnostics?

Wait, what?? Let's calculate Tolerance and VIF "by hand."

First, we'll get R<sub>j</sub><sup>2</sup> by predicting age from height. Call this `model_3`.

Next, we'll extract the R<sub>j</sub><sup>2</sup> value from the summary.

We'll then use this to calculate the Tolerance and the VIF.

```{r calc-multicollinaerity, message = FALSE, echo=FALSE, eval=FALSE}

model_3 <- lm(age ~ height, data = mr_data)
model_3_r2 <- summary(model_3)$r.squared
model_3_tol <- 1 - model_3_r2
model_3_vif <- 1/model_3_tol

```

# Write an APA-Style Summary 

This summary is based on what is now called `model_2`. Don't forget to include a confidence interval around each of your estimates!

It's helpful to create objects with each of the relevant values first. 

```{r get-values, message= FALSE, echo=FALSE}
#confidence interval
model_ci <- confint(model_2)

age_ci_lower <- round(model_ci[2,1], 2)
age_ci_upper <- round(model_ci[2,2], 2)
height_ci_lower <- round(model_ci[3,1], 2)
height_ci_upper <- round(model_ci[3,2], 2)

#omnibus test
mod_summary <- summary(model_2)

df1 <- mod_summary$fstatistic[2]
df2 <- mod_summary$fstatistic[3]
f_value <- round(mod_summary$fstatistic[1], 2)
omnibus_p_value <- pf(mod_summary$fstatistic[1], mod_summary$fstatistic[2], mod_summary$fstatistic[3], lower.tail = FALSE)
omnibus_r_squared <- round(mod_summary$r.squared, 2)
omnibus_adj_r_squared <- round(mod_summary$adj.r.squared, 2)

#individual tests

age_beta <- round(mod_summary$coefficients[2,1], 2)
age_p_value <- mod_summary$coefficients[2,4]
height_beta <- round(mod_summary$coefficients[3,1], 2)
height_p_value <- mod_summary$coefficients[3,4]


```

## Now, let's write the summary:

A multiple linear regression was conducted to examine the the effect of age and height on weight. The overall model significantly predicted weight, F(`r df1`, `r df2`) = `r f_value`, *p* = `r papaja::printp(omnibus_p_value)`. The model explained a substantial proportion of variance in weight, with an R<sup>2</sup> = `r omnibus_r_squared` and an adjusted R<sup>2</sup> = `r omnibus_adj_r_squared`. Age was not a significant predictor of weight ($\beta_1$ = `r age_beta`, 95% CI\[`r age_ci_lower`, `r age_ci_upper`\], *p* `r papaja::printp(age_p_value)`). However, height was a significant predictor of weight ($\beta_2$ = `r height_beta`, 95% CI\[`r height_ci_lower`, `r height_ci_upper`\], *p* \< `r papaja::printp(height_p_value)`), indicating that for every one-unit increase in height, weight is predicted to increase by `r height_beta` units, holding age constant. Overall, our model predicting weight from age and height found that height was a significant predictor while age was not.

---
title: "PSY515: Module 1 Lecture"
format: 
  revealjs:
    multiplex: true
    slide-number: true
    incremental: true
    theme: simple
    touch: true
    code-overflow: wrap
execute:
  echo: true
editor: visual
---

# Welcome to Quant 1 <br> (aka Statistical Methods) {.center}

::: notes
Who am I? Who are you?
:::

------------------------------------------------------------------------

## Goals of this course

::: incremental
-   Understand the basic principles that underlie the type of statistical models that are widely used in Cognitive Science

-   Learn how to select, implement, and interpret these statistical models

-   Communicate the results of statistical analyses (through text and visualization)

-   Incorporate best practices for open and reproducible research
:::

::: notes
Touched on statistics in BDS1 but mostly focused on examining data visually. Now we're going to learn about the statistics that you'll do to better understand your data and when effects are significant versus simply different due to chance.

-   Learn to select the models that are most appropriate to answer the questions that you have about your data
-   Learn to implement these models in R
-   And, once you've implemented the models, learn how to interpret each of the resulting parameters of the model

Whether you're in academia or industry, being able to communicate the results of your analyses is an important skill. We'll practice this through text, visualizations, and an oral presentation at the end of the course.

Transparent & Reproducible: Using R, creating a Data Analysis Plan (specify in advance how you'll analyze data), open data

Homeworks, quizzes, and final project are all designed with these goals in mind.
:::

------------------------------------------------------------------------

## Format

::::::: columns
:::: {.column width="50%"}
::: incremental
-   **Learn**
    -   Lectures
    -   Labs
    -   Readings
-   **Practice**
    -   Weekly Quizzes
    -   Homework
:::
::::

:::: {.column width="50%"}
::: incremental
-   **Demonstrate Mastery**
    -   Final Project
-   **Get Support**
    -   Journal Entries
:::
::::
:::::::

::: notes
There will be some overlap between the readings, lectures, and lab. This is because I find that in statistics especially, hearing the same thing multiple times and in multiple different ways and both learning about them and then implementing them in R is helpful for learning.

Quizzes are meant to test and practice what you've learned during lecture and reading. They will be given on Thursdays at the start of lab and will be very short. We will discuss quizzes immediately as a way to summarize what we learned in lecture at the beginning of lab. This means there is no opportunity to make up missed quizzes if you're late or absent. However, I will drop the lowest 2 quiz scores (missed or just lowest scores if you take them all).

Homeworks are meant to practice everything. This is where you're really going to show me that you're able to implement the analyses, interpret them, and communicate. That's why they're worth the bulk of the points in class. You can work together on homework, but anything you submit must be your own work. You can also ask for my feedback on homework.

Another way you're going to demonstrate mastery of the things you learned in class is through the final project. There is info on Canvas, and we'll go over it more thoroughly later. As in BDS 1, you will choose a dataset to analyze and it's similar in a lot of ways -- because this is the kind of thing you'll likely have do a lot in academia or industry -- but also tailored to what you will learn in this class. Our project for this class three parts: a data analysis plan where you will specify in advance how you are going to analyze your data; a research report where you motivate, carry out, and interpret these analyses; and a research presentation where you (briefly, 5min) present your findings to your classmates.

I also want to build in opportunities to get support. So, I am going to ask you to complete weekly "journal entries" where you communicate with me and tell me how things are going, what questions you have, ways that you think I might be able to support the class, or just something random. You just have to write 100 words each week, ideally about something related to class. These are graded only for completeness and you can again miss 2 entries without losing points.
:::

------------------------------------------------------------------------

## My Goals

::: incremental
-   Prepare you to both understand statistical analyses in the research you read and produce statistical analyses yourselves

-   Create situations in which you can practice these skills that will help you throughout your career

-   Challenge you to learn new things in a supportive environment

-   Work together with you to make this a high-quality learning experience

-   Give everyone an A
:::

::: notes
We talked about goals for this course, but my goals in particular:

-   prepare you to be good consumers and producers of statistical analyses
-   give you opportunities to practice these skills, in ways that will prepare you for your future careers
-   challenge you to learn new things and understand statistics, but I understand that statistics can be intimidating -- though noone here should feel intimidated, I want you to know that my goal is to support you as you learn -- communication is important!
-   with that said, we do have a Slack for this course where you can communicate with me and your fellow students -- link in sidebar on Canvas
-   similarly, while I've taught stats a lot, this is the first time that this particular version of this class has been taught, so I hope we can work together to make it a good experience for you and for future students
-   ultimately, my goal is to prepare you and support you through this class so that I can give everyone an A at the end
:::

------------------------------------------------------------------------

## When are office hours?

You tell me!

Factors to consider:

::: incremental
-   Lab sessions (that will prepare you for the homework) are on Thursdays.

-   Homework is due on Tuesdays.

-   It seems like Friday or Monday would be optimal, but I know that folks might not be on campus.

-   I can hold office hours in person and/or virtually on Mondays or virtually (only) on Fridays.
:::

------------------------------------------------------------------------

## A Quick Note about AI

::: incremental
-   The use of AI is **not permitted** in this course.

-   Why?

-   I would like you to develop a deep understanding of how to run and interpret statistical analyses. Using AI to automate coding and interpretation will not help you learn and understand these concepts. It would be a disservice to you.

:::

------------------------------------------------------------------------

# Questions?

-   Please read the rest of the syllabus on your own.

::: incremental
-   For the rest of today:

    -   Descriptive Statistics, Models, and Distributions
:::

::: notes
Read the rest of the syllabus and let me know if you have any questions (email, slack, journal entry, beginning/end of next class)

For the rest of today we'll start with our first module: Descriptive Statistics, Models, and Distributions
:::

```{r, warning=FALSE, message=FALSE, echo=FALSE}

library(tidyverse)  # for pipes and plots
library(psych)      # for several useful functions, including describe
library(here)       # for working directory
library(lsr)        # for various functions

world <- read_csv(here("data/world_happiness_2015.csv"))
```

------------------------------------------------------------------------

## Why do we describe data?

::: incremental
-   Find errors in data entry or collection

-   Understand your data

-   Explore descriptive research questions

-   Overall, there's a lot to learn from descriptive statistics.
:::

::: notes
Find errors. In my own work, for example, I was analyzing some data for a ManyBabies project and combining a bunch of data from labs around the world. There was a variable in the data that was supposed to code how long infants looked at the screen during a trial. The trial was 20 seconds long, and most of the values for this variable ranged from 0 to 20. However, I noticed that some values were much higher -- closer to 20,000. Why might this have happened?

It turned out that, while we expected labs to report how many seconds infants spent looking, some labs reported this value in milliseconds. We adjusted for that, and all was well, but if we hadn't looked at the data this could have had a huge effect.

Understand your data. What's a typical value for a variable? Are the data biased in some way? In BDS 1 you learned about assumptions for statistical tests - describing your data can help you understand if these assumptions are violated.

But there's a lot of value in just describing things as well. I'm interested in the features of infants' everyday environment that influence learning and how caregivers use multimodal communication (gestures, actions, touch in addition to speech). In one of my papers, I just reported how much of the communication infants encountered was multimodal. No statistical tests, just a number and the variation around that number. And that's useful to know independently of any kind of statistical model.

But, overall, there is a lot to learn from descriptive statistics.
:::

------------------------------------------------------------------------

## Distributions

A **distribution** is a description of the \[relative\] number of times a variable X will take each of its unique values.

```{r, fig.height=4}
#| code-fold: true
#| 

hist(world$Happiness, breaks = 30, 
     main = "Distribution of happiness scores", 
     xlab = "Happiness")
```

::: notes

When we're describing at data typically not describing one observation -- my score or your score. We want to describe a group of observations or a distribution of observations. So, we want some way of summarizing across a datset. 

For example, these data were taken from the 2015 world happiness report. This is an annual survey collected as part of the gallup world poll. 

Here, on the x axis (across the bottom) we have people's ratings of how happy feel. And, on the y axis (on the side) we have the frequency of each rating. So, about 14 people rated their happiness between 5 and 5.2. As an aside, how much data is in that bar (e.g., whether it's between 5 and 5.2 or 5 and 6 or whatever) is going to depend on the number of "breaks" that you set in your code.

If you sum up the bars, you will get the total number of observations in the dataset. Here that is 136. This is the distribution on happiness ratings in the dataset.

Our goal is statistics is to model or summarize this distribution in a meaningful way.

:::


------------------------------------------------------------------------

## Question:

If I know nothing about someone, for example a participant in the survey, but I had to guess their Happiness rating, what would be the *best* number to guess?


------------------------------------------------------------------------

## The Mean!

If I don't have any other information, then the best "model" of my dataset would be the mean or average observation.

::: notes
If I don't have any other information, then the best "model" of my dataset might be the mean or average observation.

And, if our goal in creating a statistical model is to summarize the data in a meaningful way, then the mean might be the best "model" or summary of the data when you don't have any other information. 
:::

------------------------------------------------------------------------

## Mean, $\mu$

::: nonincremental
-   The **mean** is the average. The population mean is represented by the Greek symbol $\mu$.

-   Example: a set of numbers is: `7, 5, 8, 4, 9, 3`.
:::

For a vector $x$ with length $N$, the mean $(\mu)$ of $x$ is:

$$\mu = \frac{\Sigma(x_i)}{N}=\frac{7+5+8+4+9+3}{6}=\frac{`r sum(c(7,5,8,4,9,3))`}{6}=`r mean(c(7,5,8,4,9,3))`$$

------------------------------------------------------------------------

## Properties of the mean

Example: a set of numbers is: `7, 5, 8, 4, 9, 3`. The mean of these numbers is 6.

::: incremental
-   The mean can take a value not found in the dataset.

-   Fulcrum of the data
:::

::: notes
Mean can take a value not found in the dataset - 6 is the mean, but it's not in the dataset.
:::

------------------------------------------------------------------------

## The mean is the fulcrum of the data

![](images/seesaw.png){height="\"80%"}

::: notes
The mean is the fulcrum or the balancing point of the data. For example, you can think of a seesaw like this. If there's more weight on one side, then it gets off balance. So, we're trying to balance on that fulcrum.

Draw a fulcrum on the board, put 6 in the fulcrum and 3,4,5,7,8,9 across the top. Show that distance to the left and distance to the right balances out.

Now, turn the 9 into a 15. The mean becomes 7 (3+4+5+7+8+15). Now 7 is the fulcrum with 3,4,5 to the left and 8,15 to the right. Show distance to right and left. Now there's more needed on the left to balance out the right.

So, the farther a data point is from the mean, the greater 'weight' it has. It pulls the mean in that direction. Eventually, the further it gets pulled, it's not going to do a great job as that model or summary of the data. Because it's going to get pulled closer to the outlier and away from the bulk of the data.
:::

------------------------------------------------------------------------

## Properties of the mean

Example: a set of numbers is: `7, 5, 8, 4, 9, 3`. The mean of these numbers is 6.

::: nonincremental
-   The mean can take a value not found in the dataset.

-   Fulcrum of the data
:::

::: incremental
-   The mean is strongly influenced by outliers.

-   Deviations from the mean sum to 0
:::

::: notes
Again, show on the board that deviations sum to zero.
:::

------------------------------------------------------------------------

It's important to remember that the mean of a population (or group) may not represent well some (or any) members of the population.

Example: [André-François Raffray](https://www.nytimes.com/1995/12/29/world/a-120-year-lease-on-life-outlasts-apartment-heir.html) and the French apartment

![](images/apartment.jpg){fig-align="center"}

::: notes
lawyer André-François Raffray in 1965

Agreed to pay Jeanne Calment, who was 90 years old at the time, \$2500 francs (or \$500 USD) each month and when she died, he would take possession of the apartment.

He thought he'd only have to pay for a little while because the life expectancy of French women at that time was 74.5. She was well over that.

Andre was only 45 years old. He thought he'd have to pay for a few years and then get a really good deal on the apartment.

Jeanne lived another 32 years to become the oldest person on record at 122, outliving Andre by two years. He died at 77. But, by the time he died, had paid more than twice the market value for the apartment.

So what's the lesson here? He put too much faith in the average life expectancy and it wasn't a good model or estimate for
:::

------------------------------------------------------------------------

## Other measures of central tendency

::: incremental
-   The **Mean** only one measure of *central tendency*

-   **Median** -- the middle point of the data

    -   e.g., in the set of numbers `7, 10, 8, 3, 9, 3, 12`, the median number is 8.
    -   You can see this if you write them in order: 3 3 7 **8** 9 10 12

-   **Mode** -- the number that most commonly occurs in the distribution.

    -   e.g., in the set of numbers above, the mode is 3 because it occurs twice.
:::

------------------------------------------------------------------------

## Center and spread

::: incremental
-   Distributions are most often described by their mean and **variance**.

-   Typically, these two parameters are used in common inferential techniques.

-   The mean represents the average score in a distribution. A good measure of spread will tell us something about how the typical score deviates from the mean.

-   Why can't we use the average deviation?
:::

::: notes
We've already talked about center or central tendency. Most commonly, people report the mean. But we also care about the variance of a distribution because it tells us how much a typical score deviates from the mean.

Draw two distributions on the board, one tight and one wide, label them A and B. Here are two distributions of data, and I want to guess someone's score. Because I have no other information I'm going to guess that their score is the mean. When am I more likely to be right or get close to their actual score? The tighter distribution, because more of the data is close to the mean -- there's less variance.

So, to measure spread, it seems like it would make sense to just ask how far each score is from the mean and average those values? Then I could know, on average, how far scores are from the mean. Why doesn't that work? Because of one of the properties of the mean that we discussed (look at fulcrum drawing) That is, that the deviations from the mean sum to 0, so averaging those deviations we'd get 0 and it doesn't actually tell us anything about the spread.
:::

------------------------------------------------------------------------

## Sums of squares

Our solution is to square deviations.

```{r}
x = c(7, 5, 8, 4, 9, 3)
mean(x)
(deviation = x - mean(x))
deviation^2
sum(deviation^2)
```

The sum of squared deviations is referred to as **the Sum of Squares (SS)**.

::: notes
Instead of summing the deviations, we take the sum of the *squared* deviations.

Walk through code chunks.

If you've taken a stats class before, sums of squares may sound familiar.
:::

------------------------------------------------------------------------

## Variance

We calculate the average squared deviation: this is our variance, $\sigma^2$:

```{r}
sum((x - mean(x))^2)/length(x)
```

------------------------------------------------------------------------

## Standard Deviation

**Standard deviation** $\sigma$ is the square root of the variance.

```{r}
sqrt(sum(deviation^2)/length(deviation))
```

The standard deviation is more interpretable than the variance. It can be thought of as the average distance of scores from the mean.

::: notes
Go back to distributions I drew on the board. Ask which one has the larger standard deviation (the fatter one!). It has less precision. On average, individual scores are farther away from the mean.
:::

------------------------------------------------------------------------

## Skew

Skewness characterizes **symmetry** of a distribution.

![](images/skewness.png){fig-align="center"}

::: notes
We can further characterize a distribution based on it's symmetry or skewness.

Walk through figure.

In a normal distribution, the mean, median, and mode are all relatively equal.

In a skewed distribution, both the mean and median get pulled away from the mode. The mean is pulled further.

So, if I know nothing else about a person, and the data is skewed what might be a better guess at their score? The median or mode because it falls within the bulk of the data. Often people report the median when data are skewed.

:::

------------------------------------------------------------------------

## Kurtosis

Kurtosis characterizes **tail-heaviness** of a distribution.

![](images/kurtosis.png){fig-align="center"}

::: notes
We can also characterize a distribution based on it's kurtosis. Kurtosis is really about how "heavy" the tails of a distribution are and the likelihood of outliers.

You'll sometimes hear it described as peakedness, but that's a little misleading in that the thing that really matters is the heaviness of the tails (i.e., extreme values).

-   platykurtic: too flat, more data than you'd expect in the tails, negative kurtosis
-   leptokurtic: too pointy, less data than you'd expect in the tails, positive kurtosis
-   the normal distribution is mesokurtic, so not too pointy or too flat.

Most inferential statistics assume distributions are not skewed and are mesokurtic (i.e., not too pointy or too flat).
:::

------------------------------------------------------------------------

## The Normal Distribution

![](images/paranormal-distribution.jpg){fig-align="center"}

```{r, warning=FALSE, message=FALSE, results='hide', echo=FALSE}
library(tidyverse)
library(ggpubr)
# make it pretty
colors = RColorBrewer::brewer.pal(4, "Set2")
```
:::notes
In most statistical tests we assume that we have a normal distribution. A lot of assumptions that we test for, like you did in BDS1, check for normality of the distribution.

The reason we do this is because the normal distribution has some convenient properties that make it very useful for statistics.

:::

------------------------------------------------------------------------

## Characteristics of the normal distribution

::: incremental
-   The mean and standard deviation are independent.

-   The distribution is unimodal and symmetric.

-   The area of under the curve between corresponding locations, in standard deviation units, is the same regardless of $\mu$ and $\sigma$.

    -   For example, in a normal distribution, approximately 68% of the area under the curve falls between 1 $\sigma$ below the mean and 1 $\sigma$ above mean—for every normal curve (regardless of the value of the mean and standard deviation).
:::

::: notes
What is a normal distribution? [draw on board]

Mean and standard deviation are independent. Changing the mean does not change the standard deviation and vice versa.

Example: - A city with a longer average commute time (higher mean) might also have a greater variability in commute times (higher standard deviation) due to factors like traffic congestion, longer distances, and more diverse transportation options. - A city with shorter average commute times might have more consistent commute times (lower standard deviation) due to shorter distances and less traffic.

Has one mode/peak in the center and the data are symmetric around that peak (same amount on either side)

And area under the curve is the same in standard deviation units. 
What do I mean by that?
:::

------------------------------------------------------------------------

```{r}
#| code-fold: true
#| 
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 2)) +
  stat_function(fun = function(x) dnorm(x, mean = 2, sd = .2), color = "blue") +
  stat_function(fun = function(x) dnorm(x, mean = -1.25, sd = 1), color = "red") +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density") +
  ggtitle("Normal Curves")
```

::: notes
These are all normal distributions.

They have different means, they have different standard deviations. But they are all unimodal and symmetrical, and about 68% of their data is within one standard deviation of the mean. So, they meet the requirements for a normal distribution.
:::

------------------------------------------------------------------------

All of these distributions are normal and have an equivalent area (proportion) that falls between one standard deviation below and one above their respective means.

```{r, fig.width=13, fig.height = 5}
#| code-fold: true
p1 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, mean = 0, sd = 2) , 
                xlim = c(0-2, 0+2),
                geom = "area", fill = colors[1]) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 2)) +
  geom_hline(aes(yintercept = 0)) +
  annotate("text", x = -1.5, y = 0.75, label = "Mean = 0") +
  annotate("text", x = -1.5, y = 0.65, label = "SD = 2") +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density", limits = c(0,2))+
  ggtitle("Area under the normal curve") +
  theme(text = element_text(size = 25), 
        plot.title = element_text(size = 18))

p2 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, mean = 2, sd = .2) , 
                xlim = c(2-.2, 2+.2),
                geom = "area", fill = colors[1]) +
  stat_function(fun = function(x) dnorm(x, mean = 2, sd = .2)) +
  geom_hline(aes(yintercept = 0)) +
  annotate("text", x = -1.5, y = 0.75, label = "Mean = 2") +
  annotate("text", x = -1.5, y = 0.65, label = "SD = 0.2") +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density", limits = c(0,2))+
  ggtitle("Area under the normal curve")+
  theme(text = element_text(size = 25), 
        plot.title = element_text(size = 18))

p3 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, mean = -1.25, sd = 1) , 
                xlim = c(-1.25-1, -1.25+1),
                geom = "area", fill = colors[1]) +
  stat_function(fun = function(x) dnorm(x, mean = -1.25, sd = 1)) +
  geom_hline(aes(yintercept = 0)) +
  annotate("text", x = -1.5, y = 0.75, label = "Mean = -1.25") +
  annotate("text", x = -1.5, y = 0.65, label = "SD = 1") +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density", limits = c(0,2))+
  ggtitle("Area under the normal curve")+
  theme(text = element_text(size = 25), 
        plot.title = element_text(size = 18))

ggarrange(p1, p2, p3, ncol = 3)
```

::: notes
These are the same three plots, now spread out so you can see them more easily.

So, regarless of the mean and the standard deviation, approximately 68% of the data falls within one standard deviation of the mean.

For this plot on the left this means that, the mean is 0 and sd is 2, so 68% of the data falls between 0-2 or -2 and 0+2 or 2.

In this middle plot, the mean is 2 and the standard deviation is 0.2. This means that 68% of the data falls between what two numbers? \[1.8 and 2.2\]

In the plot on the right, the mean is -1.25 and the standard deviation is 1. So, 68% of the data falls between what two numbers? \[-2.25 and 0.25\]

Different means, different standard deviations, but the same amount of data between the -1SD and +1SD

[DRAW THESE FIGURES ON THE BOARD FOR LATER]
:::

------------------------------------------------------------------------

# The Empirical Rule

In a normal distribution:

::: incremental
-   Approximately 68% of the data falls within **one** standard deviation of the mean.
-   Approximately 95% of the data falls within **two** standard deviations of the mean.
-   Approximately 99.7% of the data falls within **three** standard deviations of the mean.
:::

------------------------------------------------------------------------

# The Empirical Rule

![](images/empirical_rule.png){fig-align="center"}

::: notes
Here's an illustration of the empirical rule. 

68% - purple
green plus purple sums to 95% 
and all colors together sum to 99.7%
You also see here that the distribution is symmetrical, which means that half of the data fall on either side of the mean. 

The reason that this is convenient is because if I have a normal distribution, I can look at how many standard deviations a given score is from the mean, and I can say whether it's rare or common given the distribution. 

Let's do some practice!

Q: In this first distribution, 50% of the data is _above_ what value? A: 0
Q: What about the middle distribution? A: 2

Q: In the third distribution, 68% of the data is between what two values? A: -2.25 and 0.25
Q: What about the middle distribution? A: 1.8 and 2.2

Q: In the second distribution, 2.35% of the data is _below_ what value?
A: 1.6

Q: In the first distribution, 97.35% of the data is below what value?
A: 4

Q: In the third distribution, what percent of the data is between a raw score of -3.25 and +1.25?
A: 95%

Q: Is a score of 3 more likely in the first distribution, with a mean of 0, or the second distribution, with a mean of 2?
A: First distribution. 
Q: Why?
A: Because the standard deviation in the distribution with a mean of 2 is very small. A score of 3 is only 1.5 standard deviations from the mean in the first distribution but, even though the mean in the second is 2, a score of three is 5 standard deviations away.

:::


------------------------------------------------------------------------

## Questions?

![](images/not_normal.png){fig-align="center"}

------------------------------------------------------------------------

In general, when building statistical models, we must not forget that the aim is to understand something about the real world. Or predict, choose an action, make a decision, summarize evidence, and so on, but always about the real world, not an abstract mathematical world: our models are not the reality. Hand (2014)

------------------------------------------------------------------------
